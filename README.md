# qrdqn-quickstart


QR-DQN（Quantile Regression DQN）是由DeepMind开发的一种增强学习算法，它在经典的DQN（Deep Q-Network）算法基础上进行了改进。这种算法的核心在于使用量子回归（Quantile Regression）来预测每个动作的值分布，而不是只预测一个期望值。这使得算法可以更好地处理环境中的不确定性和变化，提高学习效率和性能。

从第一性原理出发，QR-DQN的核心步骤主要包括：

1. **量子回归表示**：在传统的DQN中，神经网络输出一个单一的预测值（即Q值）来代表每一个动作的期望回报。而在QR-DQN中，网络将输出一系列的值（量子点），这些值代表了动作值分布的不同分位点。这些分位点的数量是预先设定的，例如可以是50或100个分位点。

2. **量子回归损失函数**：QR-DQN使用量子回归损失来训练神经网络。这个损失函数考虑了预测分位点与实际回报之间的误差，使用分位数误差（Quantile Huber loss）来计算损失。这种损失函数对异常值具有较好的鲁棒性，并能有效地学习概率分布。

3. **样本更新规则**：QR-DQN使用与DQN相同的经验回放机制，但在更新样本时，它会针对每个动作的分位点分别计算损失并更新。这种方法可以更细致地调整网络对不同概率水平的预测，使其更加精确地反映可能的回报分布。

4. **探索策略**：和DQN一样，QR-DQN通常使用ε-贪婪策略进行探索，这意味着大部分时间选择当前估计最优的动作，而有一定概率（ε）随机选择其他动作，以此来探索可能的更好的策略。

5. **目标网络**：QR-DQN维护一个目标网络来稳定训练过程。这个网络的参数定期从主网络复制过来，但不参与每步的更新，其主要用于计算目标值，以减少学习过程中的震荡和不稳定性。

通过这些步骤，QR-DQN不仅可以更准确地估计动作值，而且能够捕捉到潜在的环境不确定性，提供更鲁棒的学习表现。这使其在面对具有高度随机性和复杂性的任务时表现出色。
